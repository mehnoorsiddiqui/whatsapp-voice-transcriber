/**
 * OpenAI APILib
 *
 * This file was automatically generated by APIMATIC v3.0 ( https://www.apimatic.io ).
 */

import {
  array,
  boolean,
  lazy,
  nullable,
  number,
  object,
  optional,
  Schema,
  string,
  unknown,
} from '../schema';
import {
  ChatCompletionRequestMessage,
  chatCompletionRequestMessageSchema,
} from './chatCompletionRequestMessage';

export interface CreateChatCompletionRequest {
  /** ID of the model to use. Currently, only `gpt-3.5-turbo` and `gpt-3.5-turbo-0301` are supported. */
  model: string;
  /** The messages to generate chat completions for, in the [chat format](/docs/guides/chat/introduction). */
  messages: ChatCompletionRequestMessage[];
  /**
   * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
   * We generally recommend altering this or `top_p` but not both.
   */
  temperature?: number | null;
  /**
   * An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
   * We generally recommend altering this or `temperature` but not both.
   */
  topP?: number | null;
  /** How many chat completion choices to generate for each input message. */
  n?: number | null;
  /** If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. */
  stream?: boolean | null;
  /** Up to 4 sequences where the API will stop generating further tokens. */
  stop?: unknown;
  /** The maximum number of tokens allowed for the generated answer. By default, the number of tokens the model can return will be (4096 - prompt tokens). */
  maxTokens?: number;
  /**
   * Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
   * [See more information about frequency and presence penalties.](/docs/api-reference/parameter-details)
   */
  presencePenalty?: number | null;
  /**
   * Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
   * [See more information about frequency and presence penalties.](/docs/api-reference/parameter-details)
   */
  frequencyPenalty?: number | null;
  /**
   * Modify the likelihood of specified tokens appearing in the completion.
   * Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
   */
  logitBias?: unknown | null;
  /** A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids). */
  user?: string;
}

export const createChatCompletionRequestSchema: Schema<CreateChatCompletionRequest> = object(
  {
    model: ['model', string()],
    messages: [
      'messages',
      array(lazy(() => chatCompletionRequestMessageSchema)),
    ],
    temperature: ['temperature', optional(nullable(number()))],
    topP: ['top_p', optional(nullable(number()))],
    n: ['n', optional(nullable(number()))],
    stream: ['stream', optional(nullable(boolean()))],
    stop: ['stop', optional(unknown())],
    maxTokens: ['max_tokens', optional(number())],
    presencePenalty: ['presence_penalty', optional(nullable(number()))],
    frequencyPenalty: ['frequency_penalty', optional(nullable(number()))],
    logitBias: ['logit_bias', optional(nullable(unknown()))],
    user: ['user', optional(string())],
  }
);
